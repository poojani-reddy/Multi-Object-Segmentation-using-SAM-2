Multi-Object Video Segmentation with SAM 2

A next-generation, prompt-driven segmentation system powered by Segment Anything Model 2 (SAM 2) for accurate multi-object image and video understanding.

ğŸŒŸ Project Overview

This project implements an interactive multi-object segmentation and tracking system capable of:

ğŸ” Detecting multiple objects in images

ğŸ¥ Tracking objects across video frames

ğŸ” Maintaining object identity & temporal consistency

ğŸš« Working without task-specific retraining

Unlike traditional CNN-based models that process frames independently, this system leverages:

ğŸ§  Transformer-based global context modeling

ğŸ’¾ Streaming memory for video consistency

âœ… Resulting in smooth, stable, and high-quality segmentation.

ğŸ”¥ Why This Project Matters

Traditional segmentation approaches often face:

âŒ Flickering masks

âŒ Identity switching

âŒ Poor temporal stability

âŒ Heavy retraining requirements

By integrating SAM 2â€™s memory-augmented transformer architecture, this project enables reliable segmentation suitable for:

ğŸ“¹ Surveillance Systems

ğŸ¤– Autonomous Systems

ğŸ¬ Video Analytics

ğŸ›°ï¸ Real-Time Monitoring Applications

âœ¨ Core Capabilities

âœ” Prompt-based segmentation (points, bounding boxes, masks)
âœ” Multi-object tracking across video frames
âœ” Streaming memory for temporal stability
âœ” High-quality pixel-level mask generation
âœ” No retraining required for unseen objects
âœ” Near real-time performance

ğŸ§  System Architecture

The system consists of four main components:

ğŸ”¹ Vision Transformer (ViT) Encoder

Extracts deep feature representations with global context awareness.

ğŸ”¹ Prompt Encoder

Encodes user inputs (clicks, bounding boxes, masks) into embeddings.

ğŸ”¹ Streaming Memory Module

Stores object features across video frames to maintain identity consistency.

ğŸ”¹ Mask Decoder

Generates segmentation masks along with IoU confidence scores.

ğŸ›  Technology Stack
Category	Tools Used
ğŸ Language	Python 3.10+
ğŸ”¥ Framework	PyTorch
ğŸ“š Libraries	OpenCV, NumPy, Matplotlib, TorchVision
ğŸ¤– Model	SAM 2 (Pretrained)
ğŸ’» Environment	Google Colab / Jupyter Notebook
âš¡ Hardware	NVIDIA GPU (Recommended)
ğŸš€ Getting Started
1ï¸âƒ£ Clone the Repository
git clone https://github.com/your-username/your-project-name.git
cd your-project-name
2ï¸âƒ£ Install Dependencies
pip install torch torchvision opencv-python numpy matplotlib
3ï¸âƒ£ Download SAM 2
git clone https://github.com/facebookresearch/sam2.git
ğŸ§ª Usage Guide
ğŸ“· Image Segmentation

Load an image

Provide a prompt (point / bounding box / mask)

Generate pixel-level segmentation masks

ğŸ¥ Video Segmentation & Tracking

Initialize object in the first frame using a prompt

Memory module stores object features

Objects are tracked consistently across frames

ğŸ“Š Performance Highlights

ğŸ¯ Accurate object boundary detection
ğŸ” Strong temporal consistency
ğŸ§© Reduced identity switching
ğŸ“¦ Supports multiple objects simultaneously

Input Type	Objects	Temporal Stability	Output Quality
Image	Multiple	N/A	High
Video	Multiple	High	Stable & Smooth
ğŸ“ Project Structure
sam2/               # Core SAM 2 framework
notebooks/          # Experiment notebooks
SA_V dataset/       # Dataset files
SAM2/               # Model integration scripts
training/           # Training & evaluation scripts
tools/              # Utility tools and outputs
README.md
requirements.txt
ğŸ Conclusion

This project demonstrates how transformer-based segmentation with streaming memory overcomes the limitations of traditional models, enabling:

ğŸ¯ Accurate segmentation

ğŸ” Temporal consistency

ğŸ“ˆ Scalable multi-object tracking

âš¡ Near real-time performance
