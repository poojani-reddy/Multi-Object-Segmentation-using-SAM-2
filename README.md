<div align="center">
ğŸš€ Multi-Object Video Segmentation with SAM 2
Prompt-Driven â€¢ Transformer-Based â€¢ Memory-Augmented â€¢ Real-Time
<img src="https://img.shields.io/badge/Python-3.10+-blue?style=for-the-badge&logo=python" /> <img src="https://img.shields.io/badge/PyTorch-Deep%20Learning-red?style=for-the-badge&logo=pytorch" /> <img src="https://img.shields.io/badge/Model-SAM2-green?style=for-the-badge" /> <img src="https://img.shields.io/badge/GPU-Recommended-orange?style=for-the-badge&logo=nvidia" /> </div>
âœ¨ Overview

This project implements an interactive multi-object segmentation and tracking system powered by Segment Anything Model 2 (SAM 2).

It enables:

ğŸ” Accurate multi-object image segmentation

ğŸ¥ Stable tracking across video frames

ğŸ” Temporal consistency with streaming memory

ğŸš« Zero retraining for unseen objects

Unlike traditional CNN-based models, this system uses:

ğŸ§  Transformer-based global feature modeling

ğŸ’¾ Streaming memory for cross-frame consistency

Result: Smooth, stable, high-quality segmentation in images and videos

ğŸ”¥ Why This Project Stands Out

Traditional segmentation models suffer from:

âŒ Flickering masks

âŒ Identity switching

âŒ Frame-by-frame inconsistency

âŒ Heavy retraining needs

This project solves those using memory-augmented transformer architecture, making it suitable for:

ğŸ“¹ Surveillance Systems

ğŸ¤– Autonomous Systems

ğŸ¬ Video Analytics

ğŸ›°ï¸ Real-Time Monitoring

ğŸ§  System Architecture
Input Frame + Prompt
        â†“
Vision Transformer Encoder
        â†“
Prompt Encoder
        â†“
Streaming Memory Module
        â†“
Mask Decoder
        â†“
Segmentation Masks + IoU Score
ğŸ”¹ Vision Transformer (ViT)

Extracts deep global contextual features.

ğŸ”¹ Prompt Encoder

Converts clicks, boxes, or masks into embeddings.

ğŸ”¹ Streaming Memory

Stores object features across frames for identity preservation.

ğŸ”¹ Mask Decoder

Generates pixel-level masks with confidence scores.

ğŸ¯ Core Features

âœ” Prompt-based segmentation

âœ” Multi-object tracking

âœ” Streaming memory consistency

âœ” High-quality mask generation

âœ” Works on unseen objects

âœ” Near real-time inference

ğŸ›  Tech Stack
Category	Technology
Language	Python 3.10+
Framework	PyTorch
Model	SAM 2 (Pretrained)
Libraries	OpenCV, NumPy, Matplotlib
Environment	Colab / Jupyter
Hardware	NVIDIA GPU
ğŸš€ Installation
git clone https://github.com/your-username/your-project-name.git
cd your-project-name
pip install -r requirements.txt

Download SAM 2:

git clone https://github.com/facebookresearch/sam2.git
ğŸ§ª Usage
ğŸ“· Image Segmentation

Load image

Provide prompt (point / box / mask)

Generate segmentation mask

ğŸ¥ Video Segmentation

Initialize object in first frame

Memory module stores features

Objects tracked across frames

ğŸ“Š Performance
Input	Multi-Object	Temporal Stability	Output Quality
Image	âœ…	N/A	High
Video	âœ…	High	Stable & Smooth
ğŸ“‚ Project Structure
sam2/               # Core framework
notebooks/          # Experiments
dataset/            # Data
integration/        # Model integration
training/           # Evaluation scripts
tools/              # Utilities
ğŸ Conclusion

This project demonstrates how transformer-based segmentation with streaming memory overcomes traditional video segmentation limitations, enabling:

ğŸ¯ Accurate object boundaries

ğŸ” Strong temporal consistency

ğŸ“¦ Multi-object scalability

âš¡ Real-time capable performance
