Multi-Object Video Segmentation with SAM 2

âœ¨ A next-generation prompt-driven segmentation system powered by Segment Anything Model 2 (SAM 2) for accurate multi-object image and video understanding.

ğŸŒŸ Project Overview

This project builds an interactive multi-object segmentation and tracking system capable of:

ğŸ” Detecting multiple objects in images

ğŸ¥ Tracking objects across video frames

ğŸ” Maintaining object identity & temporal consistency

ğŸš« Operating without task-specific retraining

Unlike traditional CNN-based models that process frames independently, this system leverages:

ğŸ§  Transformer-based global context modeling

ğŸ’¾ Streaming memory for long-term video consistency

âœ… Result: Smooth, stable, high-quality segmentation across frames

ğŸ”¥ Why This Project Matters

Traditional segmentation systems often suffer from:

âŒ Flickering masks

âŒ Identity switching

âŒ Poor temporal stability

âŒ Heavy retraining requirements

By integrating SAM 2â€™s memory-augmented transformer architecture, this project enables reliable segmentation for:

ğŸ“¹ Surveillance Systems

ğŸ¤– Autonomous Vehicles & Robotics

ğŸ¬ Video Analytics

ğŸ›°ï¸ Real-Time Monitoring Applications

âœ¨ Core Features

âœ” Prompt-based segmentation (points, bounding boxes, masks)
âœ” Multi-object tracking across video frames
âœ” Streaming memory for temporal stability
âœ” High-quality pixel-level mask generation
âœ” No retraining for unseen objects
âœ” Near real-time performance

ğŸ§  System Architecture

The system consists of four major components:

ğŸ”¹ 1. Vision Transformer (ViT) Encoder

Extracts deep feature representations with global contextual awareness.

ğŸ”¹ 2. Prompt Encoder

Encodes user inputs (clicks, bounding boxes, masks) into embedding vectors.

ğŸ”¹ 3. Streaming Memory Module

Stores object-level features across frames to maintain identity consistency.

ğŸ”¹ 4. Mask Decoder

Generates segmentation masks with IoU confidence scores.

ğŸ›  Technology Stack
Category	Tools
ğŸ Language	Python 3.10+
ğŸ”¥ Framework	PyTorch
ğŸ“š Libraries	OpenCV, NumPy, Matplotlib, TorchVision
ğŸ¤– Model	SAM 2 (Pretrained)
ğŸ’» Environment	Google Colab / Jupyter
âš¡ Hardware	NVIDIA GPU (Recommended)
ğŸš€ Getting Started
1ï¸âƒ£ Clone Repository
git clone https://github.com/your-username/your-project-name.git
cd your-project-name
2ï¸âƒ£ Install Dependencies
pip install torch torchvision opencv-python numpy matplotlib
3ï¸âƒ£ Download SAM 2
git clone https://github.com/facebookresearch/sam2.git
ğŸ§ª Usage Guide
ğŸ“· Image Segmentation

Load an image

Provide a prompt (point / box / mask)

Generate pixel-level segmentation masks

ğŸ¥ Video Segmentation & Tracking

Initialize object in first frame

Memory module stores object features

Objects are tracked consistently across frames

ğŸ“Š Performance Highlights

ğŸ¯ Precise object boundary detection

ğŸ” Strong temporal consistency

ğŸ§© Reduced identity switching

ğŸ“¦ Supports multiple objects simultaneously

Input	Objects	Temporal Stability	Output
Image	Multiple	N/A	High Quality
Video	Multiple	High	Stable & Smooth
ğŸ“ Project Structure
sam2/               # Core SAM 2 framework
notebooks/          # Experiment notebooks
SA_V_dataset/       # Dataset files
integration/        # Model integration scripts
training/           # Training & evaluation
tools/              # Utilities
README.md
requirements.txt
ğŸ Conclusion

This project demonstrates how transformer-based segmentation with streaming memory overcomes traditional limitations by enabling:

âœ… Accurate segmentation

âœ… Temporal consistency

âœ… Scalable multi-object tracking

âœ… Near real-time performance
